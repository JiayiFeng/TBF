#!/usr/bin/env python3
"""启动 TBF SFT 数据加载服务器。

这个脚本使用 mmq.data.sft_dataloader 中的 tbf_dataloader_start_at_batch_id 和 
tbf_to_records 函数来启动 TBF HTTP 服务器。
"""

import jsonargparse
import os
import sys
import time
from tbf.dataloader_server import TBFBatchHTTPServer
from mmq.data.sft_tbf_dataloader import tbf_dataloader_start_at_batch_id, to_records_func_creator
from functools import partial
from typing import Any

def _create_arg_parser() -> jsonargparse.ArgumentParser:
    parser = jsonargparse.ArgumentParser(description="启动 TBF SFT 数据加载服务器")
    parser.add_argument("--host", default="127.0.0.1", help="服务器监听地址 (默认: 127.0.0.1)")
    parser.add_argument("--port", type=int, default=8999, help="服务器监听端口 (默认: 8999)")
    parser.add_argument("--prefetch-count", type=int, default=2, help="预取批次数量 (默认: 2)")
    parser.add_argument("--local-rank-count", type=int, default=8, help="本地 rank 数量 (默认: 8)")
    parser.add_argument("--data-dir", default="/tmp/tbf_local", help="存放batch数据的目录 (默认: /tmp/tbf_local)")
    parser.add_argument("--page-size", type=int, default=4096, help="页面大小（字节）(默认: 4096)")

    parser.add_argument(
        "--index_file",
        type=str,
        required=True,
        help="index file, generated by mmq_prepare_pretrain_data/mmq_batch_collator",
    )
    parser.add_argument(
        "--filename_transform",
        type=str,
        help="a sed string to transform filenames in index file",
    )
    parser.add_argument(
        "--start_iteration",
        type=int,
        default=None,
        help="start iteration for this dataset",
    )
    parser.add_argument(
        "--stop_iteration",
        type=int,
        default=None,
        help="stop iteration for this dataset",
    )
    parser.add_argument(
        "--micro_batch_size",
        type=int,
        help="micro batch size splitter",
    )
    parser.add_argument(
        "--micro_batch_splitter_strategy",
        type=str,
        help="micro batch splitter strategy",
        default="hint",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        help="number worker to load train data",
        default=1,
    )
    parser.add_argument(
        "--image_filename_transform",
        type=str,
        default=None,
        help="image file transformer",
    )
    parser.add_argument(
        "--varlen_batch_concat",
        type=bool,
        default=False,
        help="concat varlen batch as a non-varlen batch",
    )
    parser.add_argument(
        "--pretrain",
        type=bool,
        default=False,
        help="pretrain mode",
    )
    parser.add_argument(
        "--extra_args",
        type=dict[str, Any],
        default=None,
        help="extra args for dataset, mainly use in eval set",
    )

    parser.add_argument(
        "--tp_size",
        type=int,
        default=1,
        help="tensor parallel size",
    )
    parser.add_argument(
        "--ap_size",
        type=int,
        default=1,
        help="attention parallel size",
    )
    parser.add_argument(
        "--ring_attn_size",
        type=int,
        default=1,
        help="ring attention parallel size",
    )
    parser.add_argument(
        "--rank_ap_mapping",
        type=list[int],
        default=None,
        help="rank to attention parallel mapping",
    )
    parser.add_argument(
        "--rank_ring_attn_mapping",
        type=list[int],
        default=None,
        help="rank to ring attention parallel mapping",
    )

    parser.add_argument(
        "--gpt.num_mtp",
        type=int,
        default=0,
        help="number of multi-token predictions for GPT",
    )
    parser.add_argument("--gpt.oe_grams", type=list[int], default=[])
    parser.add_argument("--gpt.vocab_size", type=int, required=True)
    parser.add_argument("--gpt.oe_use_simple_hash", type=bool, default=False)

    parser.add_argument(
        "--distributed.ring_impl",
        type=str,
        choices=["ring_flash", "zigzag_flex", "allgather_flash"],
        default="ring_flash",
    )

    parser.add_argument(
        "--debug.average_loss_level",
        type=str,
        default="token",
        choices=["token", "sequence"],
        help="the level to average loss, default is token",
    )



    return parser


def main():
    parser = _create_arg_parser()
    args = parser.parse_args()

    print("=" * 70)
    print("TBF SFT 数据加载服务器")
    print("=" * 70)
    print(f"配置:")
    print(f"  Host:              {args.host}")
    print(f"  Port:              {args.port}")
    print(f"  Prefetch Count:    {args.prefetch_count}")
    print(f"  Local Rank Count:  {args.local_rank_count}")
    print(f"  Data Directory:    {args.data_dir}")
    print(f"  Page Size:         {args.page_size} bytes")
    print("=" * 70)

    to_records_funcs = []
    for ap_rank in range(args.ap_size):
        funcs = []
        for ring_attn_rank in range(args.ring_attn_size):
            funcs.append(to_records_func_creator(args=args, ap_rank=ap_rank, ap_size=args.ap_size, ring_attn_rank=ring_attn_rank, ring_attn_size=args.ring_attn_size))
        to_records_funcs.append(funcs)

    if args.rank_ap_mapping is None:
        if args.ap_size > 1:
            raise ValueError("ap_size > 1 but rank_ap_mapping is not provided")
        args.rank_ap_mapping = [0] * args.local_rank_count

    if args.rank_ring_attn_mapping is None:
        if args.ring_attn_size > 1:
            raise ValueError("ring_attn_size > 1 but rank_ring_attn_mapping is not provided")
        args.rank_ring_attn_mapping = [0] * args.local_rank_count

    # 创建服务器
    server = TBFBatchHTTPServer(
        dataloader_start_at_batch_id=partial(tbf_dataloader_start_at_batch_id, dataset_args=args),
        to_records_funcs=to_records_funcs,
        rank_ap_mapping=args.rank_ap_mapping,
        rank_ring_attn_mapping=args.rank_ring_attn_mapping,
        prefetch_count=args.prefetch_count,
        local_rank_count=args.local_rank_count,
        local_dir=args.data_dir,
        page_size=args.page_size,
    )
    
    # 启动服务器
    server.start(host=args.host, port=args.port)
    
    print(f"\n✓ 服务器已启动: {server.base_url}")
    print(f"\n使用示例:")
    print(f"  - 获取当前批次ID: curl '{server.base_url}/current_batch_id?local_rank=0'")
    print(f"  - 跳转到批次:     curl -X POST '{server.base_url}/seek?batch_id=100'")
    print(f"  - 获取下一批次:   curl -X POST '{server.base_url}/fetch_next?local_rank=0'")
    print(f"\n按 Ctrl+C 停止服务器...")
    
    try:
        # 保持主线程运行
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n\n正在停止服务器...")
        server.stop()
        print("✓ 服务器已停止")


if __name__ == "__main__":
    main()
